# LangChainç‰ˆæœ¬çš„è¿è¡Œæ¨¡å—
#
# æ­¤æ¨¡å—å®ç°äº†åŸºäºLangChainæ¡†æ¶çš„tau_benchæ™ºèƒ½ä½“è¯„ä¼°
# ä¸»è¦åŠŸèƒ½ï¼š
# 1. åˆ›å»ºLangChain LLMå®ä¾‹ï¼ˆæ”¯æŒOpenAIã€DashScopeã€LiteLLMç­‰ï¼‰
# 2. è¿è¡Œtau_benchä»»åŠ¡è¯„ä¼°å¾ªç¯
# 3. æ”¶é›†å’Œä¿å­˜è¯„ä¼°ç»“æœ
# 4. è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆå¹³å‡å¥–åŠ±ã€Pass^kç­‰ï¼‰

import json
import os
import time
import random
import traceback
from typing import List
from datetime import datetime
from math import comb
import multiprocessing
from dotenv import load_dotenv

# tau_benchç±»å‹å¯¼å…¥
from tau_bench.types import EnvRunResult, RunConfig

# LangChain imports
from langchain_community.chat_models import ChatLiteLLM
from langchain_openai import ChatOpenAI

from env import EnvLangChain
from data import load_data


# ä¿®æ”¹JSONåºåˆ—åŒ–ä»¥æ­£ç¡®æ˜¾ç¤ºä¸­æ–‡å­—ç¬¦
original_dumps = json.dumps
def custom_dumps(*args, **kwargs):
    """è‡ªå®šä¹‰JSONåºåˆ—åŒ–å‡½æ•°ï¼Œç¡®ä¿ä¸­æ–‡æ­£ç¡®æ˜¾ç¤º"""
    kwargs['ensure_ascii'] = False
    return original_dumps(*args, **kwargs)
json.dumps = custom_dumps


load_dotenv(".env", override=True)

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGSMITH_API_KEY"] = os.getenv("LANGSMITH_API_KEY")
os.environ["LANGSMITH_PROJECT"] = "agent_evaluation"

# ä»ç¯å¢ƒå˜é‡åŠ è½½APIé…ç½®
API_KEY = os.getenv("DASHSCOPE_API_KEY") if os.getenv("DASHSCOPE_API_KEY") else os.environ.get('LLM_API_KEY')
API_URL = os.environ.get("LLM_BASE_URL")

# Langfuseè¿½è¸ªé…ç½®ï¼ˆå¯é€‰ï¼‰
# Langfuseæ˜¯ä¸€ä¸ªLLMåº”ç”¨çš„å¯è§‚æµ‹æ€§å¹³å°
public_key = os.environ.get("LANGFUSE_PUBLIC_KEY")
secret_key = os.environ.get("LANGFUSE_SECRET_KEY")
langfuse_endpoint = os.environ.get("LANGFUSE_HOST")

# å¦‚æœé…ç½®äº†Langfuseï¼Œè®¾ç½®è¿½è¸ªå›è°ƒå¤„ç†å™¨
if public_key and secret_key and langfuse_endpoint:
    from langfuse import CallbackHandler
    langfuse_handler = CallbackHandler(
        public_key=public_key,
        secret_key=secret_key,
        host=langfuse_endpoint
    )
else:
    langfuse_handler = None


def create_llm(config: RunConfig):
    """
    æ ¹æ®tau_bench RunConfigåˆ›å»ºLangChain LLMå®ä¾‹

    æ”¯æŒå¤šç§æ¨¡å‹æä¾›å•†ï¼š
    - OpenAIåŠå…¼å®¹APIï¼ˆä½¿ç”¨ChatOpenAIï¼‰
    - DashScopeã€Anthropicç­‰ï¼ˆä½¿ç”¨ChatLiteLLMï¼‰

    Args:
        config: tau_benchçš„RunConfigå¯¹è±¡ï¼ŒåŒ…å«æ¨¡å‹é…ç½®

    Returns:
        LangChain LLMå®ä¾‹
    """
    if config.model_provider == "openai":
        # OpenAIæˆ–å…¼å®¹çš„API
        llm = ChatOpenAI(
            model=config.model,
            temperature=config.temperature,
            base_url=API_URL,
            api_key=API_KEY,
        )
    elif config.model_provider in ["dashscope", "anthropic", "litellm"] or True:
        # ä½¿ç”¨LiteLLMæ”¯æŒå…¶ä»–æä¾›å•†
        llm = ChatLiteLLM(
            model=config.model,
            temperature=config.temperature,
            api_base=API_URL,
            api_key=API_KEY,
        )
    else:
        raise ValueError(f"Unsupported model provider: {config.model_provider}")

    return llm


def run(config: RunConfig) -> List[EnvRunResult]:
    """
    è¿è¡ŒåŸºäºLangChainçš„tau_benchæ™ºèƒ½ä½“è¯„ä¼°

    æµç¨‹ï¼š
    1. æ ¹æ®configåŠ è½½tau_benchä»»åŠ¡åˆ—è¡¨ï¼ˆtest/train/devï¼‰
    2. åˆ›å»ºLangChain LLMå®ä¾‹
    3. å¯¹æ¯ä¸ªä»»åŠ¡åˆ›å»ºEnvLangChainç¯å¢ƒå¹¶è¿è¡Œ
    4. æ”¶é›†tau_benchæ ‡å‡†çš„EnvRunResultç»“æœ
    5. è®¡ç®—å¹¶æ˜¾ç¤ºè¯„ä¼°æŒ‡æ ‡
    6. ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶

    Args:
        config: tau_benchçš„RunConfigé…ç½®å¯¹è±¡

    Returns:
        List[EnvRunResult]: tau_benchæ ‡å‡†æ ¼å¼çš„è¯„ä¼°ç»“æœåˆ—è¡¨
    """
    # éªŒè¯tau_benchæ”¯æŒçš„ç¯å¢ƒç±»å‹
    assert config.env in ["retail", "airline"], "Only retail and airline envs are supported"
    assert config.task_split in ["train", "test", "dev"], "Invalid task split"

    random.seed(config.seed)
    time_str = datetime.now().strftime("%m%d%H%M%S")
    # æ„å»ºç»“æœæ–‡ä»¶è·¯å¾„
    ckpt_path = f"{config.log_dir}/langchain-{config.agent_strategy}-{config.model.split('/')[-1]}-{config.temperature}_range_{config.start_index}-{config.end_index}_user-{config.user_model.split('/')[-1]}-{config.user_strategy}_{time_str}.json"

    if not os.path.exists(config.log_dir):
        os.makedirs(config.log_dir)

    # åŠ è½½ç³»ç»Ÿæç¤ºè¯ï¼ˆåŒ…å«ç¯å¢ƒçŸ¥è¯†åº“ï¼‰
    with open(os.path.join("./", "wiki.md"), "r") as f:
        system_prompt = f.read()

    # æ ¹æ®ä»»åŠ¡é›†åˆç±»å‹ä»tau_benchåŠ è½½ä»»åŠ¡
    match config.task_split:
        case "test":
            from tau_bench.envs.retail.tasks_test import TASKS_TEST as tasks
        case "train":
            from tau_bench.envs.retail.tasks_train import TASKS_TRAIN as tasks
        case "dev":
            from tau_bench.envs.retail.tasks_dev import TASKS_DEV as tasks
        case _:
            raise ValueError(f"Unknown task split: {config.task_split}")

    end_index = (
        len(tasks) if config.end_index == -1 else min(config.end_index, len(tasks))
    )

    results: List[EnvRunResult] = []
    lock = multiprocessing.Lock()

    if config.task_ids and len(config.task_ids) > 0:
        print(f"Running tasks {config.task_ids} (checkpoint path: {ckpt_path})")
    else:
        print(
            f"Running tasks {config.start_index} to {end_index} (checkpoint path: {ckpt_path})"
        )

    # åˆ›å»ºLangChain LLMå®ä¾‹
    llm = create_llm(config)

    total_cost = 0

    # æ‰§è¡Œå¤šè½®è¯•éªŒ
    for i in range(config.num_trials):
        if config.task_ids and len(config.task_ids) > 0:
            idxs = config.task_ids
        else:
            idxs = list(range(config.start_index, end_index))

        if config.shuffle:
            random.shuffle(idxs)

        def _run(idx: int, total_cost: float) -> tuple[EnvRunResult, float]:
            """
            è¿è¡Œå•ä¸ªtau_benchä»»åŠ¡

            Args:
                idx: tau_benchä»»åŠ¡ç´¢å¼•
                total_cost: ç´¯è®¡æˆæœ¬

            Returns:
                (EnvRunResult, æ›´æ–°åçš„æˆæœ¬)
            """
            print(f"Running task {idx}")
            try:
                # åˆ›å»ºLangChainç¯å¢ƒï¼Œä½¿ç”¨tau_benchä»»åŠ¡åˆ—è¡¨
                env = EnvLangChain(
                    tasks=tasks,  # tau_benchä»»åŠ¡åˆ—è¡¨
                    llm=llm,
                    system_prompt=system_prompt,
                    terminate_tools=["transfer_to_human_agents"],
                    task_index=idx,
                    config=config
                )

                # è¿è¡Œç¯å¢ƒå¾ªç¯ï¼ˆæ™ºèƒ½ä½“-ç”¨æˆ·æ¨¡æ‹Ÿå™¨äº¤äº’ï¼‰
                res = env.loop()
                total_cost += res.total_cost

                # æ„å»ºtau_benchæ ‡å‡†çš„EnvRunResult
                result = EnvRunResult(
                    task_id=idx,
                    reward=res.reward,
                    info=res.info,
                    traj=res.messages,
                    trial=i,
                )

            except Exception as e:
                # æ•è·å¼‚å¸¸å¹¶è®°å½•
                result = EnvRunResult(
                    task_id=idx,
                    reward=0.0,
                    info={"error": str(e), "traceback": traceback.format_exc()},
                    traj=[],
                    trial=i,
                )

            print(
                "âœ…" if result.reward == 1 else "âŒ",
                f"task_id={idx}",
                result.info,
            )
            print("-----")

            # ä¿å­˜æ£€æŸ¥ç‚¹
            with lock:
                data = []
                if os.path.exists(ckpt_path):
                    with open(ckpt_path, "r") as f:
                        data = json.load(f)
                with open(ckpt_path, "w") as f:
                    json.dump(data + [result.model_dump()], f, indent=2)

            return result, total_cost

        # é¡ºåºè¿è¡Œä»»åŠ¡ï¼ˆå¯ä¿®æ”¹ä¸ºå¹¶è¡Œæ‰§è¡Œï¼‰
        for idx in idxs:
            result, total_cost = _run(idx, total_cost)
            results.append(result)
            time.sleep(5)  # é€Ÿç‡é™åˆ¶

        print(f"Total cost: ${total_cost:.4f}")

    # æ˜¾ç¤ºè¯„ä¼°æŒ‡æ ‡
    display_metrics(results, config.num_trials)

    # ä¿å­˜æœ€ç»ˆç»“æœ
    with open(ckpt_path, "w") as f:
        json.dump([result.model_dump() for result in results], f, indent=2)
        print(f"\nğŸ“„ Results saved to {ckpt_path}\n")

    return results


def display_metrics(results: List[EnvRunResult], num_trials) -> None:
    """
    æ˜¾ç¤ºtau_benchè¯„ä¼°æŒ‡æ ‡

    è®¡ç®—å¹¶æ˜¾ç¤ºï¼š
    1. å¹³å‡å¥–åŠ±ï¼ˆAverage Rewardï¼‰
    2. Pass^kæŒ‡æ ‡ï¼šåœ¨kæ¬¡å°è¯•ä¸­è‡³å°‘æˆåŠŸä¸€æ¬¡çš„æ¦‚ç‡

    Pass^kæ˜¯ä»£ç ç”Ÿæˆå’Œæ™ºèƒ½ä½“è¯„ä¼°ä¸­å¸¸ç”¨çš„æŒ‡æ ‡
    """
    def is_successful(reward: float) -> bool:
        """åˆ¤æ–­ä»»åŠ¡æ˜¯å¦æˆåŠŸï¼ˆrewardæ¥è¿‘1.0ï¼‰"""
        return (1 - 1e-6) <= reward <= (1 + 1e-6)

    rewards = [r.reward for r in results]
    avg_reward = sum(rewards) / len(rewards)

    # è®¡ç®—æ¯ä¸ªä»»åŠ¡çš„æˆåŠŸæ¬¡æ•°
    c_per_task_id: dict[int, int] = {}
    for result in results:
        if result.task_id not in c_per_task_id:
            c_per_task_id[result.task_id] = 1 if is_successful(result.reward) else 0
        else:
            c_per_task_id[result.task_id] += 1 if is_successful(result.reward) else 0

    # è®¡ç®—Pass^kæŒ‡æ ‡
    # Pass^k = å¹³å‡æ¯ä¸ªä»»åŠ¡åœ¨kæ¬¡å°è¯•ä¸­è‡³å°‘æˆåŠŸä¸€æ¬¡çš„æ¦‚ç‡
    pass_hat_ks: dict[int, float] = {}
    for k in range(1, num_trials + 1):
        sum_task_pass_hat_k = 0
        for c in c_per_task_id.values():
            # ä½¿ç”¨ç»„åˆæ•°å­¦è®¡ç®—ï¼šä»næ¬¡å°è¯•ä¸­é€‰kæ¬¡ï¼Œè‡³å°‘æœ‰ä¸€æ¬¡æˆåŠŸçš„æ¦‚ç‡
            sum_task_pass_hat_k += comb(c, k) / comb(num_trials, k)
        pass_hat_ks[k] = sum_task_pass_hat_k / len(c_per_task_id)

    print(f"ğŸ† Average reward: {avg_reward}")
    print("ğŸ“ˆ Pass^k")
    for k, pass_hat_k in pass_hat_ks.items():
        print(f"  k={k}: {pass_hat_k}")