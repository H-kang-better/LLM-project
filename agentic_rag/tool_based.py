"""
Agentic RAG MVP ç¤ºä¾‹
å®ç°ä¸€ä¸ªæœ€å°å¯ç”¨çš„ Agentic RAG ç³»ç»Ÿï¼Œæ¼”ç¤ºå¦‚ä½•é€šè¿‡å·¥å…·ç»„åˆå®ç°"å…ˆç²—åç»†"çš„è¯æ®æ”¶é›†ç­–ç•¥
"""

from typing import List, Dict
import json
import os
from dataclasses import dataclass
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


@dataclass
class FileChunk:
    """æ–‡ä»¶ç‰‡æ®µ"""

    file_id: int
    chunk_index: int
    content: str


@dataclass
class FileInfo:
    """æ–‡ä»¶ä¿¡æ¯"""

    id: int
    filename: str
    chunk_count: int
    status: str = "done"


class MockKnowledgeBaseController:
    """æ¨¡æ‹ŸçŸ¥è¯†åº“æ§åˆ¶å™¨ - å†…å­˜ç‰ˆæœ¬ï¼Œç”¨äºæ¼”ç¤º"""

    def __init__(self):
        # æ¨¡æ‹Ÿä¸€äº›æ–‡æ¡£æ•°æ®
        self.files = [
            FileInfo(1, "rag_introduction.md", 5),
            FileInfo(2, "llm_fundamentals.md", 4),
            FileInfo(3, "vector_search.md", 3),
            FileInfo(4, "prompt_engineering.md", 4),
        ]

        # æ¨¡æ‹Ÿæ–‡æ¡£å†…å®¹ç‰‡æ®µ
        self.chunks = {
            (1, 0): FileChunk(
                1,
                0,
                "RAG (Retrieval-Augmented Generation) æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ï¼Œé€šè¿‡ä»å¤–éƒ¨çŸ¥è¯†æºæ£€ç´¢ç›¸å…³ä¿¡æ¯æ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚",
            ),
            (1, 1): FileChunk(
                1,
                1,
                "RAG çš„ä¼˜ç‚¹åŒ…æ‹¬ï¼š1) èƒ½å¤Ÿè®¿é—®æœ€æ–°ä¿¡æ¯ï¼Œ2) å‡å°‘æ¨¡å‹å¹»è§‰ï¼Œ3) æä¾›å¯è¿½æº¯çš„ä¿¡æ¯æ¥æºï¼Œ4) æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹å³å¯æ›´æ–°çŸ¥è¯†ã€‚",
            ),
            (1, 2): FileChunk(
                1,
                2,
                "RAG çš„ç¼ºç‚¹åŒ…æ‹¬ï¼š1) æ£€ç´¢è´¨é‡ç›´æ¥å½±å“ç”Ÿæˆæ•ˆæœï¼Œ2) å¢åŠ äº†ç³»ç»Ÿå¤æ‚åº¦ï¼Œ3) å¯¹å‘é‡æ•°æ®åº“çš„ä¾èµ–ï¼Œ4) å¯èƒ½å­˜åœ¨æ£€ç´¢å»¶è¿Ÿã€‚",
            ),
            (1, 3): FileChunk(
                1,
                3,
                "ä¼ ç»Ÿ RAG ç³»ç»Ÿé€šå¸¸é‡‡ç”¨å›ºå®šçš„æ£€ç´¢-ç”Ÿæˆæµç¨‹ï¼Œæ— æ³•æ ¹æ®é—®é¢˜å¤æ‚åº¦åŠ¨æ€è°ƒæ•´ç­–ç•¥ã€‚",
            ),
            (1, 4): FileChunk(
                1,
                4,
                "Agentic RAG é€šè¿‡å¼•å…¥æ™ºèƒ½ä½“ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿè‡ªä¸»å†³ç­–ä½•æ—¶æ£€ç´¢ã€å¦‚ä½•æ£€ç´¢ä»¥åŠæ£€ç´¢å¤šå°‘å†…å®¹ï¼Œä»è€Œæå‡å¤æ‚é—®é¢˜çš„å¤„ç†èƒ½åŠ›ã€‚",
            ),
            (2, 0): FileChunk(
                2,
                0,
                "å¤§è¯­è¨€æ¨¡å‹ (LLM) æ˜¯åŸºäº Transformer æ¶æ„çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡é¢„è®­ç»ƒå­¦ä¹ è¯­è¨€çš„ç»Ÿè®¡è§„å¾‹ã€‚",
            ),
            (2, 1): FileChunk(
                2, 1, "LLM çš„æ ¸å¿ƒèƒ½åŠ›åŒ…æ‹¬è‡ªç„¶è¯­è¨€ç†è§£ã€ç”Ÿæˆã€æ¨ç†å’Œå°‘æ ·æœ¬å­¦ä¹ ç­‰ã€‚"
            ),
            (2, 2): FileChunk(
                2, 2, "LLM çš„å±€é™æ€§åŒ…æ‹¬çŸ¥è¯†æˆªæ­¢æ—¶é—´ã€å¯èƒ½äº§ç”Ÿå¹»è§‰ã€è®¡ç®—èµ„æºæ¶ˆè€—å¤§ç­‰ã€‚"
            ),
            (2, 3): FileChunk(
                2,
                3,
                "å·¥å…·è°ƒç”¨æ˜¯ LLM çš„é‡è¦æ‰©å±•èƒ½åŠ›ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸å¤–éƒ¨ç³»ç»Ÿäº¤äº’ï¼Œæ‰§è¡Œå¤æ‚ä»»åŠ¡ã€‚",
            ),
            (3, 0): FileChunk(
                3,
                0,
                "å‘é‡æœç´¢æ˜¯ RAG ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œé€šè¿‡å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºæ¥å®ç°è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ã€‚",
            ),
            (3, 1): FileChunk(
                3,
                1,
                "å¸¸è§çš„å‘é‡æœç´¢ç®—æ³•åŒ…æ‹¬ FAISSã€Chromaã€Pinecone ç­‰ï¼Œå„æœ‰ä¸åŒçš„æ€§èƒ½ç‰¹ç‚¹ã€‚",
            ),
            (3, 2): FileChunk(
                3,
                2,
                "å‘é‡æœç´¢çš„æ•ˆæœå¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºembeddingæ¨¡å‹çš„è´¨é‡å’Œç´¢å¼•æ„å»ºç­–ç•¥ã€‚",
            ),
            (4, 0): FileChunk(
                4,
                0,
                "æç¤ºå·¥ç¨‹æ˜¯ä¼˜åŒ–å¤§æ¨¡å‹è¡¨ç°çš„é‡è¦æŠ€æœ¯ï¼ŒåŒ…æ‹¬è®¾è®¡æœ‰æ•ˆçš„æç¤ºæ¨¡æ¿ã€ä¸Šä¸‹æ–‡ç®¡ç†ç­‰ã€‚",
            ),
            (4, 1): FileChunk(
                4, 1, "è‰¯å¥½çš„æç¤ºè®¾è®¡åŸåˆ™åŒ…æ‹¬ï¼šæ¸…æ™°æ˜ç¡®ã€æä¾›ç¤ºä¾‹ã€ç»“æ„åŒ–è¾“å‡ºæ ¼å¼ç­‰ã€‚"
            ),
            (4, 2): FileChunk(
                4, 2, "Agent ç³»ç»Ÿçš„æç¤ºè®¾è®¡éœ€è¦è€ƒè™‘å·¥å…·è°ƒç”¨çš„ç­–ç•¥æŒ‡å¯¼å’Œé”™è¯¯å¤„ç†æœºåˆ¶ã€‚"
            ),
            (4, 3): FileChunk(
                4, 3, "ç³»ç»Ÿæç¤ºè¯åº”è¯¥æ˜ç¡®å®šä¹‰ Agent çš„è§’è‰²ã€èƒ½åŠ›è¾¹ç•Œå’Œè¡Œä¸ºè§„èŒƒã€‚"
            ),
        }

    def search(self, kb_id: int, query: str) -> List[Dict]:
        """æ¨¡æ‹Ÿè¯­ä¹‰æœç´¢ - åŸºäºå…³é”®è¯åŒ¹é…"""
        query_lower = query.lower()
        results = []

        for (file_id, chunk_idx), chunk in self.chunks.items():
            content_lower = chunk.content.lower()
            # ç®€å•çš„å…³é”®è¯åŒ¹é…è¯„åˆ†
            score = 0
            keywords = [
                "rag",
                "agentic",
                "ä¼˜ç¼ºç‚¹",
                "ä¼˜ç‚¹",
                "ç¼ºç‚¹",
                "llm",
                "æ£€ç´¢",
                "ç”Ÿæˆ",
                "å‘é‡",
                "æœç´¢",
            ]
            for keyword in keywords:
                if keyword in query_lower and keyword in content_lower:
                    score += 1

            if score > 0 or any(word in content_lower for word in query_lower.split()):
                file_info = next(f for f in self.files if f.id == file_id)
                results.append(
                    {
                        "file_id": file_id,
                        "chunk_index": chunk_idx,
                        "filename": file_info.filename,
                        "score": score + 0.5,  # åŸºç¡€åˆ†
                        "preview": chunk.content[:100] + "..."
                        if len(chunk.content) > 100
                        else chunk.content,
                    }
                )

        # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›å‰5ä¸ª
        results.sort(key=lambda x: x["score"], reverse=True)
        return results[:5]

    def getFilesMeta(self, kb_id: int, file_ids: List[int]) -> List[Dict]:
        """è·å–æ–‡ä»¶å…ƒä¿¡æ¯"""
        result = []
        for file_id in file_ids:
            file_info = next((f for f in self.files if f.id == file_id), None)
            if file_info:
                result.append(
                    {
                        "id": file_info.id,
                        "filename": file_info.filename,
                        "chunk_count": file_info.chunk_count,
                        "status": file_info.status,
                    }
                )
        return result

    def readFileChunks(self, kb_id: int, chunks: List[Dict[str, int]]) -> List[Dict]:
        """è¯»å–å…·ä½“çš„æ–‡ä»¶ç‰‡æ®µ"""
        result = []
        for chunk_spec in chunks:
            file_id = chunk_spec.get("fileId")
            chunk_index = chunk_spec.get("chunkIndex")

            chunk = self.chunks.get((file_id, chunk_index))
            if chunk:
                result.append(
                    {
                        "file_id": file_id,
                        "chunk_index": chunk_index,
                        "content": chunk.content,
                        "filename": next(
                            f.filename for f in self.files if f.id == file_id
                        ),
                    }
                )
        return result

    def listFilesPaginated(self, kb_id: int, page: int, page_size: int) -> List[Dict]:
        """åˆ†é¡µåˆ—å‡ºæ–‡ä»¶"""
        start = page * page_size
        end = start + page_size

        files_slice = self.files[start:end]
        return [
            {
                "id": f.id,
                "filename": f.filename,
                "chunk_count": f.chunk_count,
                "status": f.status,
            }
            for f in files_slice
        ]


# åˆå§‹åŒ–æ¨¡æ‹Ÿçš„çŸ¥è¯†åº“æ§åˆ¶å™¨
kb_controller = MockKnowledgeBaseController()
knowledge_base_id = 1  # æ¨¡æ‹Ÿçš„çŸ¥è¯†åº“ID


# å®šä¹‰å››ä¸ªæ ¸å¿ƒå·¥å…·
@tool("query_knowledge_base")
def query_knowledge_base(query: str) -> str:
    """Query a knowledge base with semantic search"""
    results = kb_controller.search(knowledge_base_id, query)
    return json.dumps(results, ensure_ascii=False, indent=2)


@tool("get_files_meta")
def get_files_meta(fileIds: List[int]) -> str:
    """Get metadata for files in the current knowledge base."""
    if not fileIds:
        return "è¯·æä¾›æ–‡ä»¶IDæ•°ç»„"
    results = kb_controller.getFilesMeta(knowledge_base_id, fileIds)
    return json.dumps(results, ensure_ascii=False, indent=2)


@tool("read_file_chunks")
def read_file_chunks(chunks: List[Dict[str, int]]) -> str:
    """Read content chunks from specified files in the current knowledge base."""
    if not chunks:
        return "è¯·æä¾›è¦è¯»å–çš„chunkä¿¡æ¯æ•°ç»„"
    results = kb_controller.readFileChunks(knowledge_base_id, chunks)
    return json.dumps(results, ensure_ascii=False, indent=2)


@tool("list_files")
def list_files(page: int = 0, pageSize: int = 10) -> str:
    """List all files in the current knowledge base. Returns file ID, filename, and chunk count."""
    results = kb_controller.listFilesPaginated(knowledge_base_id, page, pageSize)
    return json.dumps(results, ensure_ascii=False, indent=2)


def create_agentic_rag_system():
    """åˆ›å»º Agentic RAG ç³»ç»Ÿ"""

    # å·¥å…·æ¸…å•
    tools = [query_knowledge_base, get_files_meta, read_file_chunks, list_files]

    # è¡Œä¸ºç­–ç•¥ï¼ˆç³»ç»Ÿæç¤ºï¼‰
    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ª Agentic RAG åŠ©æ‰‹ã€‚è¯·éµå¾ªä»¥ä¸‹ç­–ç•¥é€æ­¥æ”¶é›†è¯æ®åå›ç­”ï¼š

1. å…ˆç”¨ query_knowledge_base æœç´¢ç›¸å…³å†…å®¹ï¼Œè·å¾—å€™é€‰æ–‡ä»¶å’Œç‰‡æ®µçº¿ç´¢
2. æ ¹æ®æœç´¢ç»“æœï¼Œé€‰æ‹©æœ€ç›¸å…³çš„æ–‡ä»¶ï¼Œå¯é€‰æ‹©æ€§ä½¿ç”¨ get_files_meta æŸ¥çœ‹è¯¦ç»†æ–‡ä»¶ä¿¡æ¯
3. ä½¿ç”¨ read_file_chunks ç²¾è¯»æœ€ç›¸å…³çš„2-3ä¸ªç‰‡æ®µå†…å®¹ä½œä¸ºè¯æ®
4. åŸºäºè¯»å–çš„å…·ä½“ç‰‡æ®µå†…å®¹ç»„ç»‡ç­”æ¡ˆ
5. å›ç­”æœ«å°¾ç”¨"å¼•ç”¨ï¼š"æ ¼å¼åˆ—å‡ºå®é™…è¯»å–çš„fileIdå’ŒchunkIndex

é‡è¦åŸåˆ™ï¼š
- ä¸è¦ç¼–é€ ä¿¡æ¯ï¼ŒåªåŸºäºå®é™…è¯»å–çš„ç‰‡æ®µå†…å®¹å›ç­”
- è‹¥è¯æ®ä¸è¶³ï¼Œè¯·è¯´æ˜å¹¶å»ºè®®è¿›ä¸€æ­¥æœç´¢çš„æ–¹å‘
- ä¼˜å…ˆé€‰æ‹©è¯„åˆ†é«˜çš„æœç´¢ç»“æœè¿›è¡Œæ·±å…¥é˜…è¯»
"""

    # æ¨¡å‹ä¸ Agent
    llm = ChatOpenAI(
        model=os.getenv("LLM_MODEL", "qwen-turbo"),
        temperature=0,
        max_retries=3,
        base_url=os.getenv("LLM_BASE_URL"),
        api_key=os.getenv("LLM_API_KEY")
    )


    agent = create_agent(llm, tools, system_prompt=SYSTEM_PROMPT)
    return agent


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤º Agentic RAG çš„å·¥ä½œæµç¨‹"""
    print("ğŸš€ åˆå§‹åŒ– Agentic RAG ç³»ç»Ÿ...")
    agent = create_agentic_rag_system()

    print("\nğŸ“š æ¨¡æ‹ŸçŸ¥è¯†åº“åŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š")
    for file in kb_controller.files:
        print(f"  - {file.filename} ({file.chunk_count} chunks)")

    print("\n" + "=" * 80)
    print("ğŸ’¬ å¼€å§‹é—®ç­”æ¼”ç¤º")
    print("=" * 80)

    # æµ‹è¯•é—®é¢˜
    question = "è¯·åŸºäºçŸ¥è¯†åº“ï¼Œæ¦‚è¿° RAG çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶ç»™å‡ºå¼•ç”¨ã€‚"
    print(f"\nâ“ é—®é¢˜: {question}")
    print("\nğŸ¤” Agent æ€è€ƒä¸è¡ŒåŠ¨è¿‡ç¨‹:")
    print("-" * 50)

    # è°ƒç”¨ Agent
    result = agent.invoke({"messages": [("user", question)]})

    # è¾“å‡º agent çš„æ€è€ƒè¿‡ç¨‹
    print("\nğŸ” Agent æ‰§è¡Œè¿‡ç¨‹ï¼š\n")
    for i, message in enumerate(result["messages"], 1):
        msg_type = type(message).__name__

        if msg_type == "HumanMessage":
            print(f"ğŸ‘¤ ç”¨æˆ·è¾“å…¥:")
            print(f"   {message.content}\n")

        elif msg_type == "AIMessage":
            if hasattr(message, 'tool_calls') and message.tool_calls:
                print(f"ğŸ¤– Agent å†³ç­– - è°ƒç”¨å·¥å…·:")
                for tool_call in message.tool_calls:
                    print(f"   å·¥å…·: {tool_call['name']}")
                    print(f"   å‚æ•°: {json.dumps(tool_call['args'], ensure_ascii=False)}")
                print()
            else:
                print(f"ğŸ¤– Agent æœ€ç»ˆå›ç­”:")
                print(f"   {message.content}\n")

        elif msg_type == "ToolMessage":
            print(f"ğŸ”§ å·¥å…·æ‰§è¡Œç»“æœ:")
            # æ ¼å¼åŒ–è¾“å‡ºå·¥å…·è¿”å›çš„å†…å®¹
            try:
                tool_result = json.loads(message.content)
                print(f"   {json.dumps(tool_result, ensure_ascii=False, indent=2)}\n")
            except:
                print(f"   {message.content[:200]}...\n" if len(message.content) > 200 else f"   {message.content}\n")

    print("=" * 80)
    final_answer = result["messages"][-1].content
    print("\nâœ… æœ€ç»ˆç­”æ¡ˆ:\n")
    print(final_answer)


if __name__ == "__main__":
    main()

"""
{
  'messages': [HumanMessage(content = 'è¯·åŸºäºçŸ¥è¯†åº“ï¼Œæ¦‚è¿° RAG çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶ç»™å‡ºå¼•ç”¨ã€‚', additional_kwargs = {}, response_metadata = {}, id = '96f5fd03-51bc-4e24-8645-4ce019b17283'), AIMessage(content = '', additional_kwargs = {
    'refusal': None
  }, response_metadata = {
    'token_usage': {
      'completion_tokens': 25,
      'prompt_tokens': 572,
      'total_tokens': 597,
      'completion_tokens_details': None,
      'prompt_tokens_details': {
        'audio_tokens': None,
        'cached_tokens': 0
      }
    },
    'model_provider': 'openai',
    'model_name': 'qwen-turbo',
    'system_fingerprint': None,
    'id': 'chatcmpl-d0a6f212-91f3-437a-a87c-d0b3147e5bfa',
    'finish_reason': 'tool_calls',
    'logprobs': None
  }, id = 'lc_run--4ad5980e-1b13-4843-b821-3d7e2b33ad25-0', tool_calls = [{
    'name': 'query_knowledge_base',
    'args': {
      'query': 'RAG çš„ä¼˜ç¼ºç‚¹'
    },
    'id': 'call_85976a5d2d964e008bd769',
    'type': 'tool_call'
  }], usage_metadata = {
    'input_tokens': 572,
    'output_tokens': 25,
    'total_tokens': 597,
    'input_token_details': {
      'cache_read': 0
    },
    'output_token_details': {}
  }), ToolMessage(content = '[\n  {\n    "file_id": 1,\n    "chunk_index": 2,\n    "filename": "rag_introduction.md",\n    "score": 2.5,\n    "preview": "RAG çš„ç¼ºç‚¹åŒ…æ‹¬ï¼š1) æ£€ç´¢è´¨é‡ç›´æ¥å½±å“ç”Ÿæˆæ•ˆæœï¼Œ2) å¢åŠ äº†ç³»ç»Ÿå¤æ‚åº¦ï¼Œ3) å¯¹å‘é‡æ•°æ®åº“çš„ä¾èµ–ï¼Œ4) å¯èƒ½å­˜åœ¨æ£€ç´¢å»¶è¿Ÿã€‚"\n  },\n  {\n    "file_id": 1,\n    "chunk_index": 0,\n    "filename": "rag_introduction.md",\n    "score": 1.5,\n    "preview": "RAG (Retrieval-Augmented Generation) æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ï¼Œé€šè¿‡ä»å¤–éƒ¨çŸ¥è¯†æºæ£€ç´¢ç›¸å…³ä¿¡æ¯æ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚"\n  },\n  {\n    "file_id": 1,\n    "chunk_index": 1,\n    "filename": "rag_introduction.md",\n    "score": 1.5,\n    "preview": "RAG çš„ä¼˜ç‚¹åŒ…æ‹¬ï¼š1) èƒ½å¤Ÿè®¿é—®æœ€æ–°ä¿¡æ¯ï¼Œ2) å‡å°‘æ¨¡å‹å¹»è§‰ï¼Œ3) æä¾›å¯è¿½æº¯çš„ä¿¡æ¯æ¥æºï¼Œ4) æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹å³å¯æ›´æ–°çŸ¥è¯†ã€‚"\n  },\n  {\n    "file_id": 1,\n    "chunk_index": 3,\n    "filename": "rag_introduction.md",\n    "score": 1.5,\n    "preview": "ä¼ ç»Ÿ RAG ç³»ç»Ÿé€šå¸¸é‡‡ç”¨å›ºå®šçš„æ£€ç´¢-ç”Ÿæˆæµç¨‹ï¼Œæ— æ³•æ ¹æ®é—®é¢˜å¤æ‚åº¦åŠ¨æ€è°ƒæ•´ç­–ç•¥ã€‚"\n  },\n  {\n    "file_id": 1,\n    "chunk_index": 4,\n    "filename": "rag_introduction.md",\n    "score": 1.5,\n    "preview": "Agentic RAG é€šè¿‡å¼•å…¥æ™ºèƒ½ä½“ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿè‡ªä¸»å†³ç­–ä½•æ—¶æ£€ç´¢ã€å¦‚ä½•æ£€ç´¢ä»¥åŠæ£€ç´¢å¤šå°‘å†…å®¹ï¼Œä»è€Œæå‡å¤æ‚é—®é¢˜çš„å¤„ç†èƒ½åŠ›ã€‚"\n  }\n]', name = 'query_knowledge_base', id = 'f48f8779-e44d-45e9-98b2-c7498b4faa52', tool_call_id = 'call_85976a5d2d964e008bd769'), AIMessage(content = 'RAGï¼ˆRetrieval-Augmented Generationï¼‰æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ï¼Œé€šè¿‡ä»å¤–éƒ¨çŸ¥è¯†æºæ£€ç´¢ç›¸å…³ä¿¡æ¯æ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚å…¶ä¼˜ç‚¹åŒ…æ‹¬ï¼š\n\n1. **èƒ½å¤Ÿè®¿é—®æœ€æ–°ä¿¡æ¯**ï¼šRAG å¯ä»¥ä»å¤–éƒ¨çŸ¥è¯†æºè·å–æœ€æ–°çš„æ•°æ®ï¼Œç¡®ä¿ç”Ÿæˆå†…å®¹çš„æ—¶æ•ˆæ€§ã€‚\n2. **å‡å°‘æ¨¡å‹å¹»è§‰**ï¼šé€šè¿‡æ£€ç´¢çœŸå®ä¿¡æ¯ï¼ŒRAG èƒ½å¤Ÿå‡å°‘æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„è™šæ„æˆ–ä¸å‡†ç¡®å†…å®¹ã€‚\n3. **æä¾›å¯è¿½æº¯çš„ä¿¡æ¯æ¥æº**ï¼šRAG ç”Ÿæˆçš„å†…å®¹å¯ä»¥é™„å¸¦å¼•ç”¨æ¥æºï¼Œä¾¿äºéªŒè¯å’Œè¿½æº¯ä¿¡æ¯çš„å‡†ç¡®æ€§ã€‚\n4. **æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹å³å¯æ›´æ–°çŸ¥è¯†**ï¼šRAG ç³»ç»Ÿå¯ä»¥é€šè¿‡æ›´æ–°å¤–éƒ¨çŸ¥è¯†åº“æ¥æå‡ç”Ÿæˆæ•ˆæœï¼Œè€Œæ— éœ€å¯¹æ¨¡å‹æœ¬èº«è¿›è¡Œé‡æ–°è®­ç»ƒã€‚\n\nç„¶è€Œï¼ŒRAG ä¹Ÿå­˜åœ¨ä¸€äº›ç¼ºç‚¹ï¼š\n\n1. **æ£€ç´¢è´¨é‡ç›´æ¥å½±å“ç”Ÿæˆæ•ˆæœ**ï¼šå¦‚æœæ£€ç´¢åˆ°çš„ä¿¡æ¯ä¸å‡†ç¡®æˆ–ä¸ç›¸å…³ï¼Œç”Ÿæˆçš„å†…å®¹å¯èƒ½ä¼šå—åˆ°å½±å“ã€‚\n2. **å¢åŠ äº†ç³»ç»Ÿå¤æ‚åº¦**ï¼šRAG éœ€è¦åŒæ—¶å¤„ç†æ£€ç´¢å’Œç”Ÿæˆä¸¤ä¸ªç¯èŠ‚ï¼Œè¿™ä¼šå¢åŠ ç³»ç»Ÿçš„å¤æ‚æ€§å’Œç»´æŠ¤æˆæœ¬ã€‚\n3. **å¯¹å‘é‡æ•°æ®åº“çš„ä¾èµ–**ï¼šRAG çš„æ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå‘é‡æ•°æ®åº“çš„è´¨é‡å’Œæ•ˆç‡ã€‚\n4. **å¯èƒ½å­˜åœ¨æ£€ç´¢å»¶è¿Ÿ**ï¼šç”±äºéœ€è¦ä»å¤–éƒ¨çŸ¥è¯†æºæ£€ç´¢ä¿¡æ¯ï¼ŒRAG å¯èƒ½ä¼šå‡ºç°å“åº”å»¶è¿Ÿçš„é—®é¢˜ã€‚\n\næ­¤å¤–ï¼Œä¼ ç»Ÿ RAG ç³»ç»Ÿé€šå¸¸é‡‡ç”¨å›ºå®šçš„æ£€ç´¢-ç”Ÿæˆæµç¨‹ï¼Œæ— æ³•æ ¹æ®é—®é¢˜å¤æ‚åº¦åŠ¨æ€è°ƒæ•´ç­–ç•¥ã€‚è€Œ Agentic RAG é€šè¿‡å¼•å…¥æ™ºèƒ½ä½“ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿè‡ªä¸»å†³ç­–ä½•æ—¶æ£€ç´¢ã€å¦‚ä½•æ£€ç´¢ä»¥åŠæ£€ç´¢å¤šå°‘å†…å®¹ï¼Œä»è€Œæå‡å¤æ‚é—®é¢˜çš„å¤„ç†èƒ½åŠ›ã€‚\n\nå¼•ç”¨ï¼š\n- fileId: 1, chunkIndex: 0\n- fileId: 1, chunkIndex: 1\n- fileId: 1, chunkIndex: 2\n- fileId: 1, chunkIndex: 3\n- fileId: 1, chunkIndex: 4', additional_kwargs = {
    'refusal': None
  }, response_metadata = {
    'token_usage': {
      'completion_tokens': 403,
      'prompt_tokens': 1017,
      'total_tokens': 1420,
      'completion_tokens_details': None,
      'prompt_tokens_details': {
        'audio_tokens': None,
        'cached_tokens': 0
      }
    },
    'model_provider': 'openai',
    'model_name': 'qwen-turbo',
    'system_fingerprint': None,
    'id': 'chatcmpl-6c4d096a-c129-4de0-af7d-660569d98b37',
    'finish_reason': 'stop',
    'logprobs': None
  }, id = 'lc_run--bc6cbe09-de9b-4202-ab87-a332a4f18e8b-0', usage_metadata = {
    'input_tokens': 1017,
    'output_tokens': 403,
    'total_tokens': 1420,
    'input_token_details': {
      'cache_read': 0
    },
    'output_token_details': {}
  })]
}"""